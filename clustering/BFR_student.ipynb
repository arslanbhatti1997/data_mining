{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Clustering using BFR\n",
    "Data Mining 2021/2022  \n",
    "Jordi Smit and Gosia Migut  \n",
    "Revised by Bianca Cosma\n",
    "\n",
    "**WHAT** This _optional_ lab consists of several programming exercises and insight questions. These exercises are meant to let you practice with the theory covered in: [Chapter 7][1] from \"Mining of Massive Datasets\" by J. Leskovec, A. Rajaraman, J. D. Ullman.\n",
    "\n",
    "**WHY** Practicing, both through programming and answering the insight questions, aims at deepening your knowledge and preparing you for the exam. \n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. Use [StackOverflow][2] to discuss the questions with your peers. For additional questions and feedback please consult the TAs during the assigned lab session. The answers to these exercises will not be provided.\n",
    " \n",
    "[1]: http://infolab.stanford.edu/~ullman/mmds/ch7.pdf\n",
    "[2]: https://stackoverflow.com/c/tud-cs/questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "In this exercise you will implement the BFR algorithm. This is a clustering algorithm designed for very large datasets that don't fit into memory. We will simulate the lack of memory by dividing the data in a list of lists, whereby each sub-list is a different batch that has 'supposedly' been read from disk or some other storage server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from uuid import UUID\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import sys\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: The BFR algorithm\n",
    "K-means and Hierarchical Clustering are two very well known clustering algorithms. However, both work only if the entire data set is in the main memory, which means that there is an upper limit on the amount of data they can cluster. So if we want to go beyond this upper limit we need an algorithm that doesn't need the entire data set to be in main memory. In this exercise we will look at the approach of the BFR algorithm.\n",
    "\n",
    "BFR works by summarizing the clustering data into statistical data, such as the sum, squared sum and number of data points per cluster. The algorithm uses three sets that contain cluster summaries:\n",
    "- **Discard Set**:\n",
    "Contains the summaries of the data points that are *close enough* (we'll define this later on) to one of the main clusters.\n",
    "- **Compressed Set** (also known as the set of *miniclusters*):\n",
    "Contains the summaries of the data points that are not *close enough* to one of the main clusters, but form *miniclusters* with other points that are not *close enough* to one of the main clusters.\n",
    "- **Retained Set**: \n",
    "Contains data points that are not *close enough* to one of the main clusters and not *close enough* to one of the *miniclusters* (these are not summaries, but individual data points).\n",
    "\n",
    "**BFR steps:** (as outlined in this exercise)\n",
    "1. BFR uses the first chunk of data to find the $k$ main clusters and summarizes them in **Discard Set**. Then it loops through the remaining chunks of data. \n",
    "2. For each data point in one of the remaining chunks, it will check if the data point is  *close enough* to a cluster summary in the **Discard Set**. If the data point is *close enough*, it will be added to a cluster summary in the **Discard Set**. If not, it will be added to the **Retained Set**. \n",
    "3. After we went through all the data points in a chunk, we check if we can find any new *miniclusters* by combining the clusters in the **Retained Set**, using a traditional clustering method. All the new non-singleton clusters will be summarized and added to the **Compressed Set**, while all the singleton clusters will stay in the **Retained Set**. \n",
    "4. Before we continue to the next chunk, we have to check if we don't have too many *miniclusters* in the **Compressed Set**. We can reduce the number of *miniclusters* by combining them through clustering. \n",
    "5. After we have gone through all the data, we end up with $k$ main clusters, $m$ *miniclusters* and $n$ retained data points. Because we only want $k$ clusters, we need to combine all of them, which can also be done using traditional clustering.\n",
    "\n",
    "\n",
    "After we have done all this, we end up with $k$ cluster summaries, which can be used to assign future data to the closest clusters.\n",
    "\n",
    "If you are looking for a more detailed explanation, see [this online video lecture](https://www.youtube.com/watch?v=NP1Zk8MY08k) from the authors of the book or read the corresponding section of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup\n",
    "Let's get started by creating the data structures for this problem. First of all, we need to create a class for a `DataPoint`. This class stores the location of a data point and the ID of the cluster to which the point has been assigned. We also define a function which can convert this data point to a singleton `BFRCluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPoint(object):\n",
    "    \"\"\"\n",
    "    A data point that can be clustered.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector):\n",
    "        self.vector = vector\n",
    "        self.cluster_id = None\n",
    "\n",
    "    def to_singleton_cluster(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        Cluster: A cluster with a single data point.\n",
    "        \"\"\"\n",
    "        sum_v = self.vector\n",
    "        squared_sum = sum_v ** 2\n",
    "        n_data_points = 1\n",
    "        self.cluster_id = uuid.uuid4()\n",
    "        return BFRCluster(sum_v, squared_sum, n_data_points, set([self.cluster_id]))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DataPoint(vector: {self.vector}, cluster_id: {self.cluster_id})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we import some helper functions we have already created for you:\n",
    " - `load_data`;\n",
    " - `hierarchical_clustering`.\n",
    "\n",
    "You can read their documentation using Python's `help` function, as shown below, or look at their implementation in `bfr_helper.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bfr_helper import hierarchical_clustering\n",
    "from bfr_helper import load_data\n",
    "\n",
    "# help(hierarchical_clustering)\n",
    "# help(load_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create BFR clusters\n",
    "\n",
    "Next let's create a class for the BFR cluster. This class must store both the statistical summaries of the data and be usable with hierarchical clustering. All the hierarchical clustering related logic has already been implemented in its parent class `Cluster`. You can read its documentation using `help(Cluster)` or see its implementation in `bfr_helper.py`.\n",
    "\n",
    "However, the statistical summary and BFR related logic must still be implemented. **Now it is your job to**:\n",
    " - Define the `mean` attribute;\n",
    " - Define the `variance` attribute;\n",
    " - Define the `std` attribute;\n",
    " - Finish the `is_data_point_sufficiently_close` method, used to  determine if a `DataPoint` is close enough to be added to the discard set;\n",
    " - Finish the `mahalanobis_distance` method, the distance measure used by the `is_data_point_sufficiently_close` function.\n",
    "\n",
    "We define a `DataPoint` as close enough if $MD < 3 \\cdot std_i$, for at least one $i$, where $i$ is the axis index, $MD$ is the *mahalanobis distance* and $std_i$ is the standard deviation along the $i$ axis.\n",
    "\n",
    "**Hint:** You may find the following formulas useful\n",
    " $${\\sigma_i}^2 = \\frac{SUMSQ}{N}  - \\bar{x_i}^2$$\n",
    " \n",
    " $$\\bar{x_i} = \\frac{SUM}{N}$$\n",
    " \n",
    " $$MD =\\sum_{i=1}^{N} {(\\frac{x_i - \\bar{x_i}}{\\sigma_i})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bfr_helper import Cluster\n",
    "# Uncomment the line below if you want to read the documentation\n",
    "# help(Cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BFRCluster(Cluster):\n",
    "    \"\"\"\n",
    "    A summary of multiple data points.\n",
    "    \"\"\"\n",
    "    def __init__(self, sum_v, squared_sum, n_data_points, cluster_ids):\n",
    "        # START ANSWER\n",
    "        # END ANSWER\n",
    "        \n",
    "        super().__init__(sum_v, squared_sum, n_data_points, cluster_ids, mean, variance, std)\n",
    "        \n",
    "    def is_singleton(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        bool: True if the cluster only has a single data point, false otherwise.\n",
    "        \"\"\"\n",
    "        return self.n_data_points == 1\n",
    "\n",
    "    def mahalanobis_distance(self, dp):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        dp: DataPoint: The DataPoint we are interested in.\n",
    "\n",
    "        Returns:\n",
    "        float: The mahalanobis distance between the centroid of this cluster and the given data point.\n",
    "        \"\"\"\n",
    "        # START ANSWER\n",
    "        # END ANSWER\n",
    "    \n",
    "    def is_data_point_sufficiently_close(self, dp):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        dp: DataPoint: The DataPoint we are interested in.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if the mahalanobis distance is less than 3 times the std on at least one axis, false otherwise.\n",
    "        \"\"\"\n",
    "        # START ANSWER\n",
    "        # END ANSWER\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to verify that the functions were implemented correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# Initialize 3 random data points in a 2-dimensional space.\n",
    "v = np.random.rand(3,2)\n",
    "cluster = BFRCluster(np.sum(v, axis=0, keepdims=True), np.sum(v ** 2, axis=0, keepdims=True), len(v), [uuid.uuid4()])\n",
    "\n",
    "# Check that the mean is implemented correctly.\n",
    "assert cluster.mean.shape == (1,2)\n",
    "assert np.all(np.isclose(cluster.mean[0], [0.4208509, 0.56845577], atol=0.0001))\n",
    "\n",
    "# Check that the variance is implemented correctly.\n",
    "assert cluster.variance.shape == (1,2)\n",
    "assert np.all(np.isclose(cluster.variance[0], [0.0563636, 0.10571936], atol=0.0001))\n",
    "\n",
    "# Check that the std is implemented correctly.\n",
    "assert cluster.std.shape == (1,2)\n",
    "assert np.all(np.isclose(cluster.std[0], [0.2374102, 0.32514513], atol=0.0001))\n",
    "\n",
    "# Check that mahalanobis_distance is implemented correctly.\n",
    "dp = DataPoint(np.random.rand(1,2))\n",
    "assert np.isclose(cluster.mahalanobis_distance(dp), 3.1732638628025542, atol=0.0001)\n",
    "\n",
    "inpoint = DataPoint(cluster.mean)\n",
    "outpoint = DataPoint(2 * cluster.mean)\n",
    "\n",
    "# Check that is_data_point_sufficiently_close is implemented correctly.\n",
    "assert cluster.is_data_point_sufficiently_close(inpoint)\n",
    "assert not cluster.is_data_point_sufficiently_close(outpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement the BFR algorithm\n",
    "\n",
    "In this section we'll use the previously defined data structures and functions to create the BFR algorithm. Let's get started by defining the `find_index_sufficiently_close_cluster` function. This function needs to return the index of the **first** cluster in a list that is found to be sufficiently close. If no cluster is close enough, it should return `None`. \n",
    "\n",
    "We will later use this function when iterating over the chunks of data points, to check if a data point, `dp`, is sufficiently close to one of the $k$ cluster summaries in the discard set, `k_clusters`.<br>\n",
    "**Hint:** We have already defined a function which checks if the point is close enough to some cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_sufficiently_close_cluster(k_clusters, dp):\n",
    "    \"\"\"\n",
    "    Finds the index of the first sufficiently close cluster from the given list of k clusters.\n",
    "\n",
    "    Parameters:\n",
    "    k_clusters: List[Cluster]: A list of k clusters.\n",
    "    dp: DataPoint: The data point we are interested in.\n",
    "\n",
    "    Returns:\n",
    "    Optional[int]: The index of the first sufficiently cluster in the list. \n",
    "                   Returns None if no cluster is sufficiently close.\n",
    "\n",
    "    \"\"\"\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the hyperparameters of the algorithm:\n",
    "\n",
    " - `chunk_size`: how much data we can store in a single memory scan;\n",
    " - `k`: the final amount of clusters we want;\n",
    " - `num_discard`: the number of discard clusters we'll have in the algorithm;\n",
    " - `num_new_mini`: the number of new *miniclusters* we can add during one run (i.e., how many clusters we want to get after clustering the points in the retained set);\n",
    " - `num_mini`: the number of *miniclusters* we keep between memory scans (i.e., the size of the compressed set between runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This path might be different on your local machine.\n",
    "file_path = \"data/cluster.txt\"\n",
    "\n",
    "# Algorithm hyperparameters.\n",
    "chunk_size = 35\n",
    "k = 3\n",
    "num_discard = 3\n",
    "num_new_mini = 25\n",
    "num_mini = 25\n",
    "\n",
    "data = load_data(file_path, chunk_size, create_data_point_func=DataPoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we'll implement the BFR algorithm.\n",
    "\n",
    "- For the first chunk:\n",
    "\t - Fill the discard set with `num_discard` clusters, using traditional clustering of the data points in the first chunk. Note that the hierarchical clustering method defined in `bfr_helper.py` takes a list of clusters as input, so we will first have to transform the data points into singleton clusters.\n",
    "- For each of the remaining chunks:\n",
    "     - For each data point in a chunk:\n",
    "         - If the data point is sufficiently close to a cluster in the discard set, then add it to the summary of that cluster;\n",
    "         - If the data point is not sufficiently close to any cluster in the discard set, then add it to the retained set as a singleton BFR cluster.\n",
    "\t - Combine each singleton cluster in the retained set with the singleton clusters that are closest to it, using a traditional clustering method. Add the new non-singleton *miniclusters* to the compressed set. Keep the remaining singleton clusters in the retained set.\n",
    "     - If the size of the compressed set is too large, apply traditional clustering on the summaries in the set until you get `num_mini` clusters.\n",
    "- After iterating through all chunks:\n",
    "     - Combine the discard, compressed and retained sets into the desired amount of `k` clusters.\n",
    "\n",
    "**Hints:**\n",
    " - You can combine the clusters that are closest to each other using `hierarchical_clustering`;\n",
    " - Carefully look at the functions we have defined in the previous part. Most of the logic is already defined there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard = []\n",
    "compressed = []\n",
    "retained = []\n",
    "\n",
    "for dp in data[0]:\n",
    "    # Transform the data points in the first chunk into singleton clusters.\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "# Fill the discard set with num_discard clusters, using the singleton clusters determined before.\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "# Iterate over the remaining chunks.\n",
    "for chunk in data[1:]:\n",
    "    for dp in chunk:\n",
    "        index_sufficiently_close_cluster = find_index_sufficiently_close_cluster(discard, dp)\n",
    "        if index_sufficiently_close_cluster is not None:\n",
    "            # Replace the sufficiently close cluster with the new cluster, formed by adding dp.\n",
    "            # START ANSWER\n",
    "            # END ANSWER\n",
    "        else:\n",
    "            # Transform the data point into a singleton cluster and add it to the retained set\n",
    "            # START ANSWER\n",
    "            # END ANSWER\n",
    "    \n",
    "    new_miniclusters = None\n",
    "    \n",
    "    # Find the new_miniclusters by clustering the singleton clusters in the retained set.\n",
    "    # You can use hierarchical clustering to form num_new_mini clusters.\n",
    "    # You should leave the remaining singleton clusters in the retained set, \n",
    "    # and add the non-singleton clusters to the compressed set.\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "    # Perform hierarchical clustering on the newly modified compressed set, \n",
    "    # so the number of miniclusters in the compressed set for the next iteration is num_mini.\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "\n",
    "# Combine the three sets.\n",
    "combined_summaries = discard + compressed + retained\n",
    "\n",
    "resulting_k_clusters = None\n",
    "# Further combine the summaries until there are only k, using a traditional clustering method.\n",
    "# START ANSWER \n",
    "# END ANSWER\n",
    "\n",
    "# Check that the number of resulting clusters is correct.\n",
    "assert len(resulting_k_clusters) == 3\n",
    "# Check that the cluster means are determined correctly.\n",
    "assert np.all(np.isclose(sorted(list(map(lambda cluster : cluster.mean[0], resulting_k_clusters)), key = lambda x : x[0]), \n",
    "                         [[-1.96876465, 1.4193697], [-1.9142869, -2.34254524], [2.00728041, 2.03337113]], \n",
    "                         atol=0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply the algorithm\n",
    "And we are done! The only thing left to do is to look at the final result. Run the cell below to visualize the resulting clusters. The small dots are the data points, while the diamonds are the centroids of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_dp = \".\"\n",
    "marker_cluster = \"D\"\n",
    "k = len(resulting_k_clusters)\n",
    "colors = cm.rainbow(np.linspace(0,1,k))\n",
    "\n",
    "# Plot the centroids of the clusters.\n",
    "for i, cluster in enumerate(resulting_k_clusters):\n",
    "    x = cluster.mean[:, 0]\n",
    "    y = cluster.mean[:, 1]\n",
    "    plt.scatter(x, y, marker=marker_cluster,  edgecolors='k', c=[colors[i]])\n",
    "\n",
    "# Plot the assigned data.\n",
    "for chunk in data:\n",
    "    for dp in chunk:\n",
    "        x = dp.vector[:, 0]\n",
    "        y = dp.vector[:, 1]\n",
    "        color = None\n",
    "        for i, cluster in enumerate(resulting_k_clusters):\n",
    "            if cluster.contains(dp):\n",
    "                color = colors[i]\n",
    "                break\n",
    "        assert color is not None\n",
    "        plt.scatter(x, y, marker=marker_dp, c=[color])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 1}$: This algorithm works under one major assumption. What is this assumption?\n",
    "\n",
    "$\\textbf{Question 2}$: What is the major disadvantage of this assumption?\n",
    "\n",
    "$\\textbf{Question 3}$: How many secondary memory passes does this algorithm have to make?\n",
    "\n",
    "$\\textbf{Question 4}$: Let's say we have a dataset with 3 clusters `A`, `B`, and `C`. What happens if the first chunk only has data from cluster `A`?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
