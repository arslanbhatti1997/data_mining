{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Clustering using CURE\n",
    "\n",
    "Data Mining 2021/2022  \n",
    "Jordi Smit and Gosia Migut  \n",
    "Revised by Bianca Cosma\n",
    "\n",
    "**WHAT** This _optional_ lab consists of several programming exercises and insight questions. These exercises are meant to let you practice with the theory covered in: [Chapter 7][1] from \"Mining of Massive Datasets\" by J. Leskovec, A. Rajaraman, J. D. Ullman.\n",
    "\n",
    "**WHY** Practicing, both through programming and answering the insight questions, aims at deepening your knowledge and preparing you for the exam. \n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. Use [StackOverflow][2] to discuss the questions with your peers. For additional questions and feedback please consult the TAs during the assigned lab session. The answers to these exercises will not be provided.\n",
    "\n",
    "[1]: http://infolab.stanford.edu/~ullman/mmds/ch7.pdf\n",
    "[2]: https://stackoverflow.com/c/tud-cs/questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "In the following exercises you will implement the CURE algorithm. This is a clustering algorithm designed for very large datasets that don't fit into memory. We will simulate the lack of memory by dividing the data in a list of lists, whereby each sub-list is a different batch that has 'supposedly' been read from disk or some other storage server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: CURE algorithm\n",
    "K-means and Hierarchical Clustering are two very well known clustering algorithms. Both these algorithms only work if the entire dataset is in the main memory, which means that there is an upper limit on the amount of data they can cluster. So if we want to go beyond this upper limit we need an algorithm that doesn't need the entire dataset to be in main memory. In this exercise we look at the approach of the CURE algorithm. \n",
    "\n",
    "The idea of the CURE algorithm is rather simple. We don't need the entire dataset, since most of the data is very similar. So we take a random sample of the dataset that fits into memory and we cluster this data. We then go through the remaining data points and assign them to the closest cluster.\n",
    "\n",
    "The CURE algorithm we will be using in this exercise can be summarized with the following pseudocode:\n",
    "\n",
    "```\n",
    "data_samples = sample_m_data_points_from_the_dataset()\n",
    "k_sample_clusters = cluster(data_samples, k)\n",
    "cure_clusters = []\n",
    "foreach cluster in k_sample_clusters:\n",
    "\tpoints = find_k_most_representative_points(cluster)\n",
    "\tcentroid = find_centroid(points)\n",
    "\tforeach point in points:\n",
    "\t\tmove point x% towards the centroid\n",
    "\tadd cure_cluster(points) to cure_clusters \n",
    "\n",
    "foreach dp in remaining data:\n",
    "\tassign dp to cure_cluster of the closest representative point\n",
    "```\n",
    "\n",
    "If you are looking for a more detailed explanation, see [this online video lecture](https://www.youtube.com/watch?v=JrOJspZ1CUw) from the authors of the book or read the corresponding book section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup\n",
    "Let's get started by creating the data structures for this problem.  We have already created a `Cluster` class for you. This class stores its centroid and the data points that have been assigned to it.  This class will be used for the traditional hierarchical clustering.\n",
    "You can see a summary of its class signature and its documentation using the function `help(Cluster)` or you can look at its implementation in `cure_helper.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cure_helper import Cluster\n",
    "# Uncomment the line below if you want to read the documentation\n",
    "# help(Cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we import some helper functions we have already created for you: \n",
    " - `load_data`;\n",
    " - `plot_clusters`;\n",
    " - `plot_data`;\n",
    " - `plot_cure_clusters` ;\n",
    " - `hierarchical_clustering`;\n",
    " - `find_two_closest`.\n",
    " \n",
    " \n",
    "You can read their documentation using Python's `help` function, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cure_helper import load_data\n",
    "from cure_helper import plot_clusters\n",
    "from cure_helper import plot_data\n",
    "from cure_helper import plot_cure_clusters\n",
    "from cure_helper import hierarchical_clustering\n",
    "from cure_helper import find_two_closest\n",
    "from cure_helper import find_centroid\n",
    "\n",
    "# help(load_data)\n",
    "# help(plot_clusters)\n",
    "# help(plot_data)\n",
    "# help(plot_cure_clusters)\n",
    "# help(hierarchical_clustering)\n",
    "# help(find_two_closest)\n",
    "# help(find_centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create CURE clusters\n",
    "\n",
    "Let's define the `CureCluster` class. This class has two attributes, namely the `k_most_representative_points` and `data` (a `Cluster` which contains the data points that have been assigned to the `CureCluster`). The class is almost finished. The only thing left to do is to finish the `distance` function.  \n",
    "**Note:** in this exercise, we will be working with 2-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CureCluster:\n",
    "    def __init__(self, k_most_representative_points):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        k_most_representative_points: np.ndarray: A k x 2 matrix, as in this exercise we are working \n",
    "                                                  with 2-dimensional data.\n",
    "        \"\"\"\n",
    "        assert isinstance(k_most_representative_points, np.ndarray)\n",
    "        assert len(k_most_representative_points.shape) == 2 \n",
    "        self.k_most_representative_points = k_most_representative_points\n",
    "        self.data = None\n",
    "        \n",
    "    def distance(self, cluster):\n",
    "        \"\"\"\n",
    "        Calculates the distance between the centroid of the cluster passed to the method \n",
    "        and the closest representative point in this CureCluster.\n",
    "\n",
    "        Parameters:\n",
    "        cluster: Cluster: The cluster for which we need to find the minimum distance between \n",
    "                          its centroid and the representative points of this CureCluster.\n",
    "\n",
    "        Returns:\n",
    "        float: Returns the distance as a float.\n",
    "        \"\"\"\n",
    "        min_dist = sys.float_info.max\n",
    "        \n",
    "        # START ANSWER\n",
    "        # END ANSWER\n",
    "        \n",
    "        return min_dist\n",
    "    \n",
    "    def append(self, cluster):\n",
    "        \"\"\"\n",
    "        Adds the data points in the input cluster to this CureCluster.\n",
    "        Is stateful.\n",
    "\n",
    "        Parameters:\n",
    "        cluster: Cluster: A cluster that contains the data points we want to add.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            self.data = cluster\n",
    "        else:\n",
    "            self.data = self.data.merge(cluster)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"CureCluster(\\nrepresentative_points:\\n{self.k_most_representative_points},\\ndata: \\n{self.data}\\n)\\n\"\n",
    "\n",
    "# Check that your distance function works on a simple example.\n",
    "test_cluster = Cluster(np.array([[4, 3], [3, 3], [3, 3.5], [3, 4], [5, 4], [5, 4.5], [4, 4]]))\n",
    "test_cure_cluster = CureCluster(np.array([[1, 1], [3, 1], [2, 2]]))\n",
    "assert np.isclose(test_cure_cluster.distance(test_cluster), 2.5274008589934476, atol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement the CURE algorithm\n",
    "Next, let's define the `find_k_most_representative_points` function. We'll use this function to find the $k$ most representative points in a cluster.\n",
    "\n",
    "The $k$ most representative points of a cluster should be chosen such that they are far away from one another. One way to do this is to follow the first method described in subsection 7.3.2 of the book, which also includes the pseudocode and an example. First, we will pick one point at random and add it to the list of representative points. As long as the size of this list is less than $k$, we will keep adding points from the cluster to this list, each time selecting the point with the largest minimum distance to the representative points in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_point_with_largest_minimum_distance(cluster, representative_points):\n",
    "    \"\"\"\n",
    "    For the given cluster, this function should find the point in the cluster whose\n",
    "    minimum distance to the representative points is the largest.\n",
    "    \n",
    "    Parameters:\n",
    "    cluster: Cluster: The cluster we are interested in.\n",
    "    representative_points: A list of representative points in the cluster.\n",
    "\n",
    "    Returns:\n",
    "    np.array: A numpy array with two elements, representing the coordinates of the point.\n",
    "    \"\"\"\n",
    "    max_distance = -np.inf\n",
    "    point_with_max_distance = None\n",
    "    \n",
    "    # START ANSWER\n",
    "    # END ANSWER  \n",
    "    \n",
    "    return point_with_max_distance\n",
    "        \n",
    "        \n",
    "def find_k_most_representative_points(cluster, k, seed):\n",
    "    \"\"\"\n",
    "    Finds the k most representative points of a given cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    cluster: Cluster: The cluster we are interested in.\n",
    "    k: int: The amount of representative points.\n",
    "    seed: int: A random seed.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Returns a k x 2 matrix, where each row contains a representative point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pick the first point randomly and add it to the set of the k most representative points.\n",
    "    random.seed(seed)\n",
    "    first_point = cluster.data[random.randint(0, cluster.data.shape[0] - 1)]\n",
    "    k_most_representative_points = [first_point]\n",
    "    \n",
    "    # Determine the remaining (k-1) representative points, by making use of the function\n",
    "    # you implemented before.\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "    return np.array(k_most_representative_points)\n",
    "\n",
    "# Check that your method to find the k most representative points works on a simple example.\n",
    "resulting_k_representative_points = np.sort(find_k_most_representative_points(test_cluster, 3, 42), axis=0)\n",
    "assert np.array_equal(resulting_k_representative_points, \n",
    "                      [[3, 3], [4, 4], [5, 4.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll combine the previously defined functions, and write the function `transform_into_cure_cluster`, which converts a `Cluster` into a `CureCluster`. It's your job to find the `k_most_representative_points` and to move them before we create the `CureCluster`.<br>\n",
    "**Hint**: Carefully look at the pseudo code of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_into_cure_cluster(cluster, representative_points, move_to_center_percentage, seed):\n",
    "    \"\"\"\n",
    "    Transforms a given cluster into a CureCluster by:\n",
    "    1. selecting the k most representative points from the data points in the cluster.\n",
    "    2. moving the k representative points towards the centroid of the cluster by a given percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    cluster: Cluster: The cluster we want to transform.\n",
    "    representative_points: int: The number of representative_points.\n",
    "    move_to_center_percentage: float: A value between 0 and 1, \n",
    "                                      representing how much the k points should be moved towards their centroid.\n",
    "    seed: int: A random seed, to be passed to find_k_most_representative_points.\n",
    "\n",
    "    Returns:\n",
    "    CureCluster: Returns a new CureCluster with its representative points.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert 0 < move_to_center_percentage < 1, \"The value of move_to_center_percentage must be in the range (0,1)\"\n",
    "    if representative_points > len(cluster):\n",
    "        print(f\"[Warning] representative_points has been changed to {len(cluster)} for this cluster.\")\n",
    "        representative_points = len(cluster)\n",
    "    \n",
    "    k_most_representative_points = None\n",
    "    \n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "    cure_cluster = CureCluster(k_most_representative_points)\n",
    "    cure_cluster.append(cluster)\n",
    "    return cure_cluster\n",
    "\n",
    "# Check that your implementation works on a simple example.\n",
    "assert np.allclose(np.sort(transform_into_cure_cluster(test_cluster, 3, 0.2, 42).k_most_representative_points, axis=0),\n",
    "                   [[3.17142857, 3.14285714], [3.97142857, 3.94285714], [4.77142857, 4.34285714]], atol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define the `find_cure_cluster_with_min_dist` function. It's your job to find and return the `CureCluster` in the input list that is closest to the input `Cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cure_cluster_with_min_dist(cure_clusters, cluster):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    cure_clusters: List[CureCluster]: The cure clusters we want to compare against.\n",
    "    cluster: Cluster: The cluster we are interested in.\n",
    "\n",
    "    Returns:\n",
    "    CureCluster: Returns the CureCluster with the minimum distance to the given cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    cure_cluster_with_min_dist = None\n",
    "    \n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "    return cure_cluster_with_min_dist\n",
    "\n",
    "# Check that your implementation works on a simple example.\n",
    "test_cure_cluster2 = CureCluster(np.array([[0.5, 0.5], [0, 0], [-0.3, -0.3]]))\n",
    "list_of_test_cure_clusters = [test_cure_cluster, test_cure_cluster2]\n",
    "assert find_cure_cluster_with_min_dist(list_of_test_cure_clusters, test_cluster) == test_cure_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply the algorithm\n",
    "\n",
    "These are the hyperparameters of the algorithm:\n",
    "\n",
    " - `seed`: A random seed to ensure that the random sampling returns the same result between different runs;\n",
    " - `sample_size`: The amount of random samples we use to find the $k$ clusters;\n",
    " - `representative_points`: The number of representative points we take from the $k$ clusters;\n",
    " - `n_clusters`: The number of clusters we want to find at the end of the CURE algorithm;\n",
    " - `move_to_center_percentage`: How much the k representative points will be moved towards their centroid.\n",
    " \n",
    "We have two sets: `data/cluster.txt` and `data/cluster_lines.txt`. Try to find the correct hyperparameters for both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURE parameters\n",
    "seed = 42\n",
    "\n",
    "# Set the remaining 4 hyperparameters.\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "# Data set 1.\n",
    "file_path = \"data/cluster.txt\"\n",
    "\n",
    "# Data set 2.\n",
    "# file_path = \"data/cluster_lines.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(file_path)\n",
    "plot_clusters(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's sample some random points. \n",
    "\n",
    "Your task is to make sure that you have enough samples in each cluster. If this does not seem to be the case you might want to change your hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "data_sample = random.sample(data, sample_size)\n",
    "plot_clusters(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have a good, well distributed random sample from the data. \n",
    "\n",
    "Now let's perform traditional hierarchical clustering on this sample of the data set. Visually, the resulting clusters should resemble the ones we were able to distinguish in the plot of the complete data set.\n",
    "\n",
    "Depending on the dataset you are working with, you may have to pick a different distance measure to be used for the traditional clustering. You can choose between \"closest_point\" and \"mean_squared_distance\". Try to see which one works best for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a distance measure.\n",
    "distance_measure=\"mean_squared_distance\"\n",
    "\n",
    "# Cluster samples with hierarchical clustering.\n",
    "sample_clusters = hierarchical_clustering(data_sample, k=n_clusters, distance_measure=distance_measure)\n",
    "print(\"The resulting clusters of the sample data:\")\n",
    "plot_clusters(sample_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the clusters to create $k$ CURE clusters with the functions you have implemented. Then let's loop through all the data and let's assign each data point to the correct CURE cluster. On the plot you will see data points marked with dots and representative points marked with diamonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CURE Clusters.\n",
    "cure_clusters = [transform_into_cure_cluster(cluster, representative_points, move_to_center_percentage, seed) for cluster in sample_clusters]\n",
    "\n",
    "# Assign remaining data points to the clusters.\n",
    "for dp in data:\n",
    "    cure_cluster = find_cure_cluster_with_min_dist(cure_clusters, dp)\n",
    "    cure_cluster.append(dp) \n",
    "\n",
    "plot_cure_clusters(cure_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have implemented everything correctly and you have chosen some good hyperparameters, then your results from the CURE version should be similar to the result of the traditional hierarchical clustering function you see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_set_results = hierarchical_clustering(data, k=n_clusters, distance_measure=distance_measure)\n",
    "plot_clusters(full_data_set_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 1}$: What is the advantage of CURE over the BFR algorithm?\n",
    "\n",
    "$\\textbf{Question 2}$: What happens if the `sample_size` hyperparameter is too high or too low?\n",
    "\n",
    "$\\textbf{Question 3}$: What happens if the `representative_points` hyperparameter is too high or too low?\n",
    "\n",
    "$\\textbf{Question 4}$: When performing hierarchical clustering of the initially sampled points, you could choose between 2 distance measures. What is the effect of different distance measures on the final result?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
