{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Link Analysis\n",
    "Data Mining 2021/2022  \n",
    "Danny Plenge and Gosia Migut  \n",
    "Revised by Bianca Cosma\n",
    "\n",
    "**WHAT** This *optional* lab consists of several programming exercises and insight questions. \n",
    "These exercises are meant to let you practice with the theory covered in: [Chapter 5][1] from \"Mining of Massive Datasets\" by J. Leskovec, A. Rajaraman, J. D. Ullman.\n",
    "\n",
    "**WHY** Practicing, both through programming and answering the insight questions, aims at deepening your knowledge and preparing you for the exam.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. Use [StackOverflow][2]\n",
    "to discuss the questions with your peers. For additional questions and feedback please consult the TAs during the assigned lab session. The answers to these exercises will not be provided.\n",
    "\n",
    "[1]: http://infolab.stanford.edu/~ullman/mmds/ch5.pdf\n",
    "[2]: https://stackoverflow.com/c/tud-cs/questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "You will develop an algorithm which will let you rank Internet pages (and other objects) based on their relative importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: PageRank\n",
    "\n",
    "PageRank, named after _web pages_ and co-founder of Google Larry Page, was designed to combat the growing number of term spammers. In this exercise we will look at the algorithm and some of its adaptations. In the end, we will use PageRank to compute which airports in the US are most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start this exercise with a small network, simulating the entire Internet with a few sites. Then we will simulate what a random surfer would do on this network and where it is most likely to end up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Investigate the data\n",
    "\n",
    "Investigate the data of transitions from one vertex to the other in the example below. The data is of the form:\n",
    "\n",
    "```\n",
    "source|destination|weight\n",
    "```\n",
    "\n",
    "In this case, all weights are set to 1, meaning that all transitions are equally likely to happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`example` data:**\n",
    "\n",
    "```\n",
    "A|C|1  \n",
    "A|D|1  \n",
    "B|A|1  \n",
    "B|D|1  \n",
    "C|A|1   \n",
    "D|B|1  \n",
    "D|C|1  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\textbf{Question 1}$: Draw the directed graph based on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 2}$: Write out the transition matrix for this network. Verify that all columns sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 3}$: If we initialize a random surfer at a random location, what are the chances for this random surfer to be at a certain location after one iteration? Manually calculate the probabilities for all locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Parse the data\n",
    "\n",
    "Create a PageRank object and import the data from the given example. Print the data object to see how the data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# This path might be different on your local machine.\n",
    "example = 'data/example.txt'\n",
    "\n",
    "def import_data(example): \n",
    "    \"\"\"\n",
    "    This function loads the given datasets in an OrderedDict Object and\n",
    "    can be used for the next steps in this assignment.\n",
    "    :param example: The input file containing the (example) data.\n",
    "    :return: An OrderedDict containing an OrderedDict for each data point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data.\n",
    "    lines = [line.rstrip('\\n') for line in open(example)]\n",
    "    \n",
    "    # Initialize data structure.\n",
    "    data = OrderedDict()\n",
    "    for l in lines:\n",
    "        line = l.split(\"|\")\n",
    "        data[line[0]] = OrderedDict()\n",
    "    \n",
    "    # START ANSWER \n",
    "    # END ANSWER\n",
    "    \n",
    "    return data\n",
    " \n",
    "\n",
    "data = import_data(example)\n",
    "# Check that your code works on the example.\n",
    "assert data == OrderedDict([('A', OrderedDict([('A', 0), ('B', 0), ('C', 1), ('D', 1)])),\n",
    "                            ('B', OrderedDict([('A', 1), ('B', 0), ('C', 0), ('D', 1)])),\n",
    "                            ('C', OrderedDict([('A', 1), ('B', 0), ('C', 0), ('D', 0)])),\n",
    "                            ('D', OrderedDict([('A', 0), ('B', 1), ('C', 1), ('D', 0)]))])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement `construct_transition_matrix`\n",
    "\n",
    "Next, a transition matrix has to be constructed, by creating the function: `construct_transition_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def construct_transition_matrix(data):\n",
    "    \"\"\"\n",
    "    This function returns a transition_matrix based on the given data.\n",
    "    Note: you can convert an OrderedDict object to a list of (key, value) tuples with OrderedDict_Object.items().\n",
    "    :param data: The OrderedDict containing the input data.\n",
    "    :return: A two-dimensional array representing the transition matrix.\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((len(data), len(data)))\n",
    "    \n",
    "    # START ANSWER\n",
    "    # END ANSWER           \n",
    "    return matrix\n",
    "\n",
    "trans_matrix = construct_transition_matrix(data)\n",
    "# Check that all columns of the matrix sum up to 1.\n",
    "column_sums = np.sum(trans_matrix, axis=0)\n",
    "assert np.all(np.isclose(column_sums, 1.0))\n",
    "trans_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 4}$: Is the output matrix from the function `construct_transition_matrix` the same as the matrix you calculated in question 1.2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement `get_random_surfer`\n",
    "\n",
    "Finish the `get_random_surfer` function, which should create a row vector of length equal to the number of vertices in the data. Each element should represent an equal probability, and the vector elements should sum up to 1. In other words, it should construct the following vector:\n",
    "\n",
    "<center>$v = \\begin{bmatrix}\\dfrac{1}{n} \\\\ \\dfrac{1}{n} \\\\ . \\\\ . \\\\ . \\\\ \\dfrac{1}{n}\\end{bmatrix}$</center>  \n",
    "  \n",
    "Where $n$ is the number of vertices in the data, and $dim(v) = n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_surfer(data):\n",
    "    \"\"\"\n",
    "    This function returns a row vector of length equal to the number of vertices in the given data. \n",
    "    :param data: The OrderedDict containing the input data.\n",
    "    :return: An array where each value has the same probability summing up to 1.\n",
    "    \"\"\"\n",
    "    result = np.zeros((len(data), 1))\n",
    "    \n",
    "    # START ANSWER\n",
    "    # END ANSWER   \n",
    "    \n",
    "    return result\n",
    "\n",
    "random_surfer = get_random_surfer(data)\n",
    "# Check that all probabilities are equal and sum up to 1.\n",
    "assert np.all(np.isclose(random_surfer, random_surfer[0])) and np.isclose(np.sum(random_surfer), 1.0)\n",
    "\n",
    "random_surfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement `calculate_page_rank`\n",
    "\n",
    "Now complete the `calculate_page_rank` function. This function should calculate a transition matrix, get a random surfer vector and multiply these for a number of iterations. The iterative step is:  \n",
    "\n",
    "<center>$v' = Mv$</center>  \n",
    "\n",
    "Where $M$ is the transition matrix.\n",
    "\n",
    "Run the `calculate_page_rank` function on the example dataset with 10 iterations. Verify that the result is approximately as follows:  \n",
    "\n",
    "<center>$v_{10} = \\begin{bmatrix}A \\\\ B \\\\ C \\\\ D\\end{bmatrix} = \\begin{bmatrix}0.354 \\\\ 0.119 \\\\ 0.294 \\\\ 0.233\\end{bmatrix}$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_page_rank(data, trans_matrix, iterations):\n",
    "    \"\"\"\n",
    "    This function calculates the page rank based on the given data,\n",
    "    a given transition matrix (trans_matrix) and a given amount of iterations.\n",
    "    :param data: The OrderedDict containing the input data.\n",
    "    :param trans_matrix: The transition matrix.\n",
    "    :param iteration: The amount of iterations.\n",
    "    :return: A dictionary containing the PageRank for each data item.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize result.\n",
    "    result = dict()\n",
    "    \n",
    "    # START ANSWER    \n",
    "    # END ANSWER   \n",
    "\n",
    "    return result\n",
    "\n",
    "page_ranks = calculate_page_rank(data, trans_matrix, 10)\n",
    "# Check that the page ranks are approximately equal to the given values.\n",
    "assert np.isclose(page_ranks['A'], 0.354, atol=0.001) and np.isclose(page_ranks['B'], 0.119, atol=0.001) \\\n",
    "        and np.isclose(page_ranks['C'], 0.294, atol=0.001) and np.isclose(page_ranks['D'], 0.233, atol=0.001)\n",
    "\n",
    "page_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the PageRank for given data\n",
    "\n",
    "Now run the `calculate_page_rank` function on the `data/example2.txt` dataset with 10 iterations.   \n",
    "  \n",
    "**`example2` data:**  \n",
    "```\n",
    "A|C|1  \n",
    "A|D|1  \n",
    "B|A|1  \n",
    "B|D|1  \n",
    "C|C|1   \n",
    "D|B|1  \n",
    "D|C|1  \n",
    "```\n",
    "\n",
    "As you can see, this dataset is slightly different. The edge from C to A is replaced by an edge from C to C itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This path might be different on your local machine.\n",
    "example2 = 'data/example2.txt'\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "new_page_rank = calculate_page_rank(data2, trans_matrix2, 10)\n",
    "# Check that the page rank of C changes accordingly.\n",
    "assert np.isclose(new_page_rank['C'], 1.0, atol = 0.05)\n",
    "\n",
    "new_page_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 5}$: Explain the results you now get from the PageRank algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Add taxation to PageRank\n",
    "\n",
    "In order to make sure nodes like these do not corrupt our results, we can use taxation to allow the random surfer to randomly jump from one page to another. This comes down to changing our iterative step to:\n",
    "\n",
    "<center>$v' = \\beta Mv + \\dfrac{(1 - \\beta)e}{n}$</center>  \n",
    "\n",
    "Where $e$ is a vector of all ones, $n$ is the number of vertices in the data and $\\beta$ is a constant.  \n",
    "Implement the function `taxation_page_rank` which calculates this modified PageRank value using the iterative step. You may set $\\beta$ to 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxation_page_rank(data, trans_matrix, beta, iterations):\n",
    "    \"\"\"\n",
    "    This function calculates the page rank using taxation based on the initial data \n",
    "    of import_data, a given transitionMatrix (trans_matrix), a given beta for the \n",
    "    taxation and a given amount of iterations.\n",
    "    :param data: The OrderedDict containing the input data.\n",
    "    :param trans_matrix: The transition matrix.\n",
    "    :param beta: The beta.\n",
    "    :param iterations: The amount of iterations.\n",
    "    :return: A dictionary containing the PageRank for each data item.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize result.\n",
    "    result = dict()\n",
    "    \n",
    "    # START ANSWER    \n",
    "    # END ANSWER   \n",
    "    return result\n",
    "\n",
    "taxed_page_rank = taxation_page_rank(data2, trans_matrix2, 0.8, 10)\n",
    "# Check that taxation lowers the page rank of C.\n",
    "assert taxed_page_rank['C'] < new_page_rank['C']\n",
    "\n",
    "taxed_page_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 6}$: Are the results better using the `taxation_page_rank` function? What happens if we lower the beta? What happens if we increase the beta?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Use PageRank on the airport network\n",
    "\n",
    "Check out the `data/flight_data.txt` file.  \n",
    "\n",
    "First 10 rows of `flight_data`:\n",
    "\n",
    "```\n",
    "Cincinnati, OH|Omaha, NE|1\n",
    "Cincinnati, OH|Los Angeles, CA|56\n",
    "Cincinnati, OH|Milwaukee, WI|26\n",
    "Cincinnati, OH|Charlotte, NC|123\n",
    "Cincinnati, OH|Raleigh/Durham, NC|50\n",
    "Cincinnati, OH|Nashville, TN|50\n",
    "Cincinnati, OH|Chicago, IL|353\n",
    "Cincinnati, OH|Fort Myers, FL|34\n",
    "Cincinnati, OH|Orlando, FL|87\n",
    "Cincinnati, OH|San Francisco, CA|25\n",
    "```\n",
    "\n",
    "This file contains information regarding airports in the US and flights between them. Each line represents a connection from one airport to another with the weight equal to the number of flights in January 2013. Run the algorithm on this dataset for 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# This path might be different on your local machine.\n",
    "example3 = 'data/flight_data.txt'\n",
    "\n",
    "data3 = None\n",
    "trans_matrix3 = None\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "flights_page_rank = taxation_page_rank(data3, trans_matrix3, 0.8, 10)\n",
    "expected_result = ['Pago Pago, TT', 'Rockford, IL', 'Trenton, NJ', 'Staunton, VA', 'North Bend/Coos Bay, OR']\n",
    "result = list(islice({k: v for k, v in sorted(flights_page_rank.items(), key=lambda item: item[1])}.keys(), 5))\n",
    "assert result == expected_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 7}$: What is the most important airport according to the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
