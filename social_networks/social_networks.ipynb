{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Mining Social-Network Graphs\n",
    "Data Mining 2021/2022  \n",
    "Ruben Wiersma and Gosia Migut  \n",
    "Revised by Bianca Cosma\n",
    "\n",
    "**WHAT** This *optional* lab consists of several programming exercises and insight questions. \n",
    "These exercises are meant to let you practice with the theory covered in: [Chapter 10][1] from \"Mining of Massive Datasets\" by J. Leskovec, A. Rajaraman, J. D. Ullman.\n",
    "\n",
    "**WHY** Practicing, both through programming and answering the insight questions, aims at deepening your knowledge and preparing you for the exam. \n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. Use [StackOverflow][2] to discuss the questions with your peers. For additional questions and feedback please consult the TAs during the assigned lab session. The answers to these exercises will not be provided.\n",
    "\n",
    "[1]: http://infolab.stanford.edu/~ullman/mmds/ch10.pdf\n",
    "[2]: https://stackoverflow.com/c/tud-cs/questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "In this exercise, we will practice with Spectral Clustering to analyse social networks. To this end, we will create an adjacency matrix, set up a Laplacian matrix, compute the eigenvalue decomposition and perform clustering. Finally, we will use the code that you developed to cluster a large social network graph into more than two clusters.\n",
    "\n",
    "**Note:** The aim of this lab is to give you a deeper understanding of spectral clustering. To that end, it will require you to do solve some mathematical equations and dust off your linear algebra skills. So take out your pen and paper and be ready to write out the math related parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Spectral Graph Clustering\n",
    "\n",
    "You are working for a popular social networking site, FriendBase. Your managers have thought of a wonderful new feature: InstaGroups<sup>TM</sup>. InstaGroups<sup>TM</sup> will automatically suggest a clustering of your group of friends, so you can easily send messages to- and post memes meant only for a select group.\n",
    "\n",
    "In order to start working on this problem, you are given a small dataset with nine people (given as a list) and their friendships. The friendships are provided in a list of tuples, where each tuple in the list represents a friendship, e.g.: `('Albert', 'Bob')` represents a friendship between Albert and Bob. Each friendship is undirected, so a friendship defined for `('Albert', 'Bob')` will also hold for `('Bob', 'Albert')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(object):\n",
    "    \"\"\"\n",
    "    Very simple graph class that holds a list of nodes and a list of edges connecting the nodes.\n",
    "    Feel free to extend the functionality of this class to better organise your code.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nodes=[], edges=[]):\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of people, the nodes in the graph.\n",
    "people = ['Alice',\n",
    "           'Bob',\n",
    "           'Claudia',\n",
    "           'Dennis',\n",
    "           'Ellie',\n",
    "           'Frida',\n",
    "           'George',\n",
    "           'Harry',\n",
    "           'Irene']\n",
    "\n",
    "# Friendships between people, the edges in the graph.\n",
    "friendships = [('Alice', 'Bob'),\n",
    "               ('Alice', 'Claudia'),\n",
    "               ('Alice', 'Dennis'),\n",
    "               ('Bob', 'Claudia'),\n",
    "               ('Bob', 'Dennis'),\n",
    "               ('Bob', 'Frida'),\n",
    "               ('Claudia', 'Dennis'),\n",
    "               ('Claudia', 'Irene'),\n",
    "               ('Dennis', 'Ellie'),\n",
    "               ('Ellie', 'Frida'),\n",
    "               ('Ellie', 'George'),\n",
    "               ('Frida', 'George'),\n",
    "               ('Frida', 'Harry'),\n",
    "               ('George', 'Harry')]\n",
    "\n",
    "friend_graph = Graph(people, friendships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 1}$: Draw a graph of this network for yourself to visualise the network. What are the nodes? What are the edges in your graph? \n",
    "  \n",
    "$\\textbf{Question 2}$: Based on your drawing, how many clusters would you create? Which cluster would each person be assigned to? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Building the adjacency matrix\n",
    "\n",
    "We will now apply Spectral Clustering to this problem. To do so, we will need an adjacency matrix of this social network.\n",
    "\n",
    "Remember that the adjacency matrix $\\mathbf{A}$ is an $n \\times n$ matrix, where $n$ is the number of nodes in your graph. The entry at row $i$ and column $j$ is 1 if there is an edge between node $i$ and node $j$. We denote this as $a_{ij} = 1$, otherwise, $a_{ij} = 0$.\n",
    "\n",
    "Construct the adjacency matrix for the provided dataset.\n",
    "\n",
    "**Hint:** you can use `list.index(element)` to find the index of a given element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "def create_adjacency_matrix(graph):\n",
    "    \"\"\"\n",
    "    Creates and returns the adjacency matrix for a given graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    adjacency_matrix = np.zeros((len(graph.nodes), len(graph.nodes)))\n",
    "\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "    return adjacency_matrix\n",
    "\n",
    "adjacency_matrix = create_adjacency_matrix(friend_graph)\n",
    "# Check that the matrix is symmetric.\n",
    "assert np.array_equal(adjacency_matrix, np.transpose(adjacency_matrix))\n",
    "# Check Claudia's connections.\n",
    "assert np.array_equal(adjacency_matrix[2], [1., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
    "\n",
    "adjacency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the graph Laplacian\n",
    "\n",
    "The next step is to compute the Laplacian matrix given this adjacency matrix. The Laplacian of a graph is defined as\n",
    "\n",
    "$$L = D - A$$\n",
    "\n",
    "where $D$ is the degree matrix which describes the number of edges for each node on the diagonal of an $n \\times n$ matrix\n",
    "$$d_{ii} = \\sum_{j \\in \\delta(i)} 1$$\n",
    "\n",
    "Complete the provided functions to compute the Laplacian of the graph for the FriendBook dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_degree_matrix(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Computes the degree matrix from an adjacency matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    degree_matrix = np.zeros_like(adjacency_matrix)\n",
    "    \n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "    return degree_matrix\n",
    "\n",
    "def compute_laplacian(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Computes the Laplacian matrix from an adjacency matix.\n",
    "    \"\"\"\n",
    "    \n",
    "    laplacian = np.zeros_like(adjacency_matrix)\n",
    "    \n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "    return laplacian\n",
    "\n",
    "laplacian = compute_laplacian(adjacency_matrix)\n",
    "# Check that the Laplacian matrix is symmetric.\n",
    "assert np.array_equal(laplacian, np.transpose(laplacian))\n",
    "# Check the diagonal values of the Laplacian matrix.\n",
    "assert np.array_equal(np.diagonal(laplacian), np.sum(adjacency_matrix, axis=1))\n",
    "\n",
    "laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to grow your intuition of the Laplacian matrix, we will do a small exercise. Let's say you know the height of every person in the dataset. You can store these heights in a vector $\\mathbf{v}$ of length $n$. The $i$<sup>th</sup> element of the vector, $v_i$, stores the height for person $i$.\n",
    "\n",
    "$\\textbf{Question 3}$: What happens if you multiply the Laplacian matrix with the heights of each person, i.e. $\\mathbf{L}\\mathbf{v}$? Try doing this with a very small graph consisting of three nodes: a, b, c, where (a, b) and (a, c) are connected and the heights of a, b, and c are 2, 3, and 4, respectively.\n",
    "\n",
    "$\\textbf{Question 4}$: What if all people have the same height, e.g. $\\mathbf{v} = \\mathbf{1} = [1, 1, 1, ...]^T$?    \n",
    "  \n",
    "$\\textbf{Question 5}$: What are you computing this way?  \n",
    "**Hint:** What's the difference in height between each pair? How would you compute the average of these differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Eigenvalue decomposition\n",
    "\n",
    "The next step in the Spectral Clustering algorithm is to compute the eigenvalue decomposition of the Laplacian matrix. If you would like to better understand eigenvalues and eigenvectors, watch [this video on eigenvectors][1].\n",
    "\n",
    "Compute the eigenvalue decomposition of the Laplacian matrix and print the eigenvalues and eigenvectors corresponding to the *first three* eigenvalues in increasing order.\n",
    "\n",
    "**Hint:** To get the eigenvalues and eigenvectors, you should use the [`np.linalg.eigh`][2] function. It returns an array of eigenvalues in ascending order, and a matrix whose columns are the corresponding eigenvectors. `eigh` is faster than [`eig`][3] (which works for arbitrary matrices) but it should only be called with a real **symmetric** matrix. You have to make sure the input matrix you pass to this function is symmetric (but it will be in our case).\n",
    "\n",
    "Your first three eigenvalues should be approximately:\n",
    "- $0.0$\n",
    "- $0.527$\n",
    "- $1.213$\n",
    "\n",
    "[1]: https://www.youtube.com/watch?v=PFDu9oVAE-g&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=13\n",
    "[2]: https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html\n",
    "[3]: https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as la\n",
    "from numpy.testing import assert_array_equal\n",
    "\n",
    "# You should replace these values with those computed by the eigh function.\n",
    "# v will be the array of eigenvectors, and w will be the eigenvector matrix.\n",
    "v, w = None, None\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "assert np.all(np.isclose(v, [-5.98479599e-16, 0.52726226, 1.21252584, 2.434483515, 3.99999999, 4, 4.60413038, 5.32364198, 5.89795599], atol=0.000001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 6}$: What is the first eigenvalue and its corresponding eigenvector? Did you expect this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Perform Spectral Clustering\n",
    "You will now perform the final step in the Spectral Clustering algorithm: the actual clustering. Use the second eigenvector of the Laplacian matrix.\n",
    "\n",
    "Each person is assigned to a cluster based on the sign of their entry in the eigenvector. For example: if we have eigenvector $[3, 7, -2, 4, -3, 5, 2, -8, 1]^T$, we know that Alice, Bob, Dennis, Frida, George, and Irene should be in one cluster and that Claudia, Ellie, and Harry are in the other.\n",
    "\n",
    "Complete the following functions to create and print the cluster assignment in a readable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 7}$: Compare the results to your answer to $\\textbf{Question 1}$. Did you get the same clustering?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_eigenvector(laplacian):\n",
    "    \"\"\"\n",
    "    Returns the eigenvector corresponding to the second eigenvalue, as a row.\n",
    "    Note that the eigenvalues are given in ascending order.\n",
    "    \"\"\"\n",
    "    v, w = la.eigh(laplacian)\n",
    "    \n",
    "    second_w = np.ones((laplacian.shape[0], 1))\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return second_w\n",
    "\n",
    "def spectral_cluster(g):\n",
    "    \"\"\"\n",
    "    Clusters a graph given by nodes and edges using spectral clustering.\n",
    "    Returns two graphs given by nodes1, edges1 and nodes2, edges2, respectively.\n",
    "    \"\"\"\n",
    "    nodes1, edges1 = [], []\n",
    "    nodes2, edges2 = [], []\n",
    "    \n",
    "    adjacency_matrix = create_adjacency_matrix(g)\n",
    "    laplacian = compute_laplacian(adjacency_matrix)\n",
    "    \n",
    "    second_w = second_eigenvector(laplacian)\n",
    "    \n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "    g1 = Graph(nodes1, edges1)\n",
    "    g2 = Graph(nodes2, edges2)\n",
    "    \n",
    "    return g1, g2\n",
    "        \n",
    "graph1, graph2 = spectral_cluster(friend_graph)\n",
    "assert (np.array_equal(np.sort(graph1.nodes), ['Alice', 'Bob', 'Claudia', 'Dennis', 'Irene']) \\\n",
    "        and np.array_equal(np.sort(graph2.nodes), ['Ellie', 'Frida', 'George', 'Harry'])) \\\n",
    "        or (np.array_equal(np.sort(graph2.nodes), ['Alice', 'Bob', 'Claudia', 'Dennis', 'Irene']) \\\n",
    "        and np.array_equal(np.sort(graph1.nodes), ['Ellie', 'Frida', 'George', 'Harry']))\n",
    "\n",
    "print(\"Cluster 1 nodes:\", graph1.nodes)\n",
    "print(\"Cluster 2 nodes:\", graph2.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's also print the edges in the two graphs.  \n",
    "**Note:** Make sure all edges between the nodes in a cluster are there, and there are no edges connecting the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster 1 edges:\")\n",
    "for edge in graph1.edges:\n",
    "    print(edge)\n",
    "\n",
    "print()\n",
    "print(\"Cluster 2 edges:\")\n",
    "for edge in graph2.edges:\n",
    "    print(edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Implement recursive clustering\n",
    "Now we will extend this method to create partitions greater than $2$. As described in the lecture, there are two possible ways to proceed:\n",
    "1. Recursively partition each cluster using the spectral clustering algorithm until you have reached $k$ partitions.\n",
    "2. Use $d$ eigenvectors to construct a $d$-dimensional space and apply a classical clustering algorithm.\n",
    "\n",
    "The first technique is quite straightforward, so let's implement it right away and see how it performs. For now, we will limit our implementation to $k$ being powers of $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "def recursive_cluster_k(g, k):\n",
    "    assert k <= len(g.nodes)\n",
    "    assert log2(k) % 1 == 0\n",
    "    \n",
    "    depth = log2(k)\n",
    "    return recursive_cluster(g, depth)\n",
    "\n",
    "def recursive_cluster(g, depth):\n",
    "    \"\"\"\n",
    "    Recursively clusters graph g until depth is 0.\n",
    "    If you want, you can also implement this method iteratively (with a for loop).\n",
    "    \"\"\"\n",
    "    # Base case\n",
    "    if depth == 0:\n",
    "        return [g]\n",
    "     \n",
    "    clusters = []\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "        \n",
    "    return clusters\n",
    "        \n",
    "clusters = recursive_cluster_k(friend_graph, 4)\n",
    "# Check that the clusters have the correct nodes.\n",
    "assert frozenset(map(lambda x : frozenset(x.nodes), clusters)) == frozenset([frozenset(['Alice', 'Bob', 'Dennis']),\n",
    "                                                                             frozenset(['Claudia', 'Irene']),\n",
    "                                                                             frozenset(['Ellie', 'George']),\n",
    "                                                                             frozenset(['Frida', 'Harry'])])\n",
    "for cluster in clusters:\n",
    "    print(cluster.nodes)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Construct a subspace out of the eigenvectors\n",
    "\n",
    "Now for the second technique: Use $d$ eigenvectors to construct a $d$-dimensional space and apply a classical clustering algorithm. Let's break up this sentence to see what it means.\n",
    "\n",
    "1. _Use $d$ eigenvectors to construct a $d$-dimensional space._\n",
    "\n",
    "The $d$ eigenvectors you select are the bases of a $d$-dimensional space (check out the video linked in exercise 3 to see why). In order to get the coordinates of a point in this new space, all we need to do is compute the dot product between the vector representation of that point in the original space and the eigenvector corresponding to each dimension. The vector representation $\\mathbf{v}$ of a node $i$ in the original space is an $n \\times 1$ vector with all zeros and $v_i = 1$.\n",
    "\n",
    "If you compute the dot product with each eigenvector, you get $d$ 'coordinates' for that node in the new $d$-dimensional space, which is called the spectral transform.\n",
    "\n",
    "We can use a quick shortcut to get the coordinates for each node in the graph: we can simply concatenate the $d$ eigenvectors into an $n \\times d$ matrix (try writing it out). The dataset of new features will look like this: [$w_1$, $w_2$, ..., $w_d$], where $w_j$ is the $j$-th eigenvector.\n",
    "\n",
    "2. _Apply a classical clustering algorithm._\n",
    "\n",
    "Now that you have $d$ coordinates for each node, we can feed these new coordinates (or features) into a standard clustering algorithm, like hierarchical clustering or k-means. We will not implement this step but use an existing algorithm for [agglomerative clustering][1] from scikit-learn library. We will also plot the new coordinates. Of course, you can also use another clustering algorithm or your own implementation from the Machine Learning course.\n",
    "\n",
    "Let's get started! We will use only two eigenvectors, so we can plot the new coordinates. So we will call the function you implemented, `spectral_coordinates`, with `d`=2.  \n",
    "**Note:** Remember that we don't use the very first eigenvector as it consists of only 1s.\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "\n",
    "def spectral_coordinates(laplacian, d):\n",
    "    \"\"\"\n",
    "    Returns an n x d matrix, where n is the number of rows in the laplacian matrix. \n",
    "    Each row in this matrix represents the spectral coordinates of a point in a d-dimensional space.\n",
    "    \"\"\"\n",
    "    v, w = la.eigh(laplacian)\n",
    "    \n",
    "    coordinates = np.ones((laplacian.shape[0], d))\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return coordinates\n",
    "\n",
    "def spectral_cluster_k(g, k):    \n",
    "    adjacency_matrix = create_adjacency_matrix(g)\n",
    "    laplacian = compute_laplacian(adjacency_matrix)\n",
    "    \n",
    "    coordinates = spectral_coordinates(laplacian, 2)\n",
    "    \n",
    "    for i in range(len(coordinates)):\n",
    "        plt.scatter(coordinates[i, 0], coordinates[i, 1])\n",
    "    plt.legend(people)\n",
    "    plt.show()\n",
    "    \n",
    "    return coordinates\n",
    "\n",
    "coordinates = spectral_cluster_k(friend_graph, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 8}$: What do the axes mean in this plot?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can investigate how the clusters were assigned.\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=4).fit_predict(coordinates)\n",
    "\n",
    "for index, cluster in enumerate(clustering):\n",
    "    print(\"{} belongs to cluster {}\".format(people[index], cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 9}$: Did you get the same clustering results using the two methods? If yes, will that always be the case? If no, what is the difference?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
